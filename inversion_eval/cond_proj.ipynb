{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782b8d62-b895-4d97-800f-d9e2615358cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 12:31:25.385301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745141485.415792  226143 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745141485.425195  226143 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from diffusers import LCMScheduler, DDPMScheduler, StableDiffusionPipeline\n",
    "# import ImageReward as RM\n",
    "# from torchmetrics.functional.multimodal import clip_score\n",
    "# from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision.transforms.functional import to_tensor, resize\n",
    "from diffusers.models.attention_processor import AttnProcessor2_0\n",
    "\n",
    "from utils.loading import load_models\n",
    "from utils import p2p, generation, inversion\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "NUM_REVERSE_CONS_STEPS = 4\n",
    "REVERSE_TIMESTEPS = [259, 519, 779, 999]\n",
    "NUM_FORWARD_CONS_STEPS = 4\n",
    "FORWARD_TIMESTEPS = [19, 259, 519, 779]\n",
    "NUM_DDIM_STEPS = 50\n",
    "START_TIMESTEP = 19\n",
    "\n",
    "def generate_images_batch(solver, reverse_cons_model, prompts, latent):\n",
    "    images = []\n",
    "    generator = torch.Generator(device=\"cuda:0\").manual_seed(42)\n",
    "    controller = p2p.AttentionStore()\n",
    "    images, gen_latent, latents = generation.runner(\n",
    "        guidance_scale=0.0,\n",
    "        tau1=1.0,\n",
    "        tau2=1.0,\n",
    "        is_cons_forward=True,\n",
    "        model=reverse_cons_model,\n",
    "        dynamic_guidance=False,\n",
    "        w_embed_dim=512,\n",
    "        start_time=50,\n",
    "        solver=solver,\n",
    "        prompt=prompts,\n",
    "        controller=controller,\n",
    "        generator=generator,\n",
    "        latent=latent,\n",
    "        return_type=\"image\",\n",
    "        # num_inference_steps=50,\n",
    "    )\n",
    "    return images, gen_latent, latents\n",
    "\n",
    "\n",
    "def invert_images_batch(solver, prompts, images, guidance_scale, use_reverse_model=False):\n",
    "    (image_gt, image_rec), latents, uncond_embeddings, latent_orig = inversion.invert(\n",
    "        is_cons_inversion=True,\n",
    "        # do_npi=False,\n",
    "        # do_nti=True,\n",
    "        w_embed_dim=512,\n",
    "        stop_step=50,  # from [0, NUM_DDIM_STEPS]\n",
    "        inv_guidance_scale=guidance_scale,\n",
    "        dynamic_guidance=False,\n",
    "        tau1=0.0,\n",
    "        tau2=0.0,\n",
    "        solver=solver,\n",
    "        images=images,\n",
    "        prompt=prompts,\n",
    "        # num_inner_steps=10,\n",
    "        # early_stop_epsilon=1e-5,\n",
    "        seed=42,\n",
    "        use_reverse_model=use_reverse_model\n",
    "    )\n",
    "\n",
    "    return image_gt, image_rec, latents[-1], latents, latent_orig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcd1d3ce-66de-4eaf-a150-f8998eb89e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7abec510549494e8c4a8ed0e084aea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward CD is initialized with guidance embedding, dim 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of UNet2DConditionModel were not initialized from the model checkpoint at sd-legacy/stable-diffusion-v1-5 and are newly initialized: ['time_embedding.cond_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded model is loading from checkpoints/sd15_cfg_distill.pt\n",
      "Reverse CD is loading from checkpoints/iCD-SD1.5_259_519_779_999.safetensors\n",
      "Forward CD is loading from checkpoints/iCD-SD1.5_19_259_519_779.safetensors\n",
      "Endpoints reverse CTM: tensor([259, 519, 779, 999]), tensor([519, 779, 999,   0])\n",
      "Endpoints forward CTM: tensor([ 19, 259, 519, 779]), tensor([259, 519, 779, 999])\n"
     ]
    }
   ],
   "source": [
    "ldm_stable, reverse_cons_model, forward_cons_model = load_models(\n",
    "    model_id=\"sd-legacy/stable-diffusion-v1-5\",\n",
    "    device=\"cuda:0\",\n",
    "    forward_checkpoint=\"checkpoints/iCD-SD1.5_19_259_519_779.safetensors\",\n",
    "    reverse_checkpoint=\"checkpoints/iCD-SD1.5_259_519_779_999.safetensors\",\n",
    "    r=64,\n",
    "    w_embed_dim=512,\n",
    "    teacher_checkpoint=\"checkpoints/sd15_cfg_distill.pt\",\n",
    "    dtype=\"fp16\",\n",
    ")\n",
    "# ldm_stable.unet.set_attn_processor(AttnProcessor2_0())\n",
    "# reverse_cons_model.unet.set_attn_processor(AttnProcessor2_0())\n",
    "# forward_cons_model.unet.set_attn_processor(AttnProcessor2_0())\n",
    "\n",
    "ldm_stable.set_progress_bar_config(disable=True)\n",
    "reverse_cons_model.set_progress_bar_config(disable=True)\n",
    "forward_cons_model.set_progress_bar_config(disable=True)\n",
    "\n",
    "ldm_stable.safety_checker = None\n",
    "reverse_cons_model.safety_checker = None\n",
    "forward_cons_model.safety_checker = None\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"scheduler\",\n",
    ")\n",
    "\n",
    "solver = generation.Generator(\n",
    "    model=ldm_stable,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    n_steps=NUM_DDIM_STEPS,\n",
    "    forward_cons_model=forward_cons_model,\n",
    "    forward_timesteps=FORWARD_TIMESTEPS,\n",
    "    reverse_cons_model=reverse_cons_model,\n",
    "    reverse_timesteps=REVERSE_TIMESTEPS,\n",
    "    num_endpoints=NUM_REVERSE_CONS_STEPS,\n",
    "    num_forward_endpoints=NUM_FORWARD_CONS_STEPS,\n",
    "    max_forward_timestep_index=49,\n",
    "    start_timestep=START_TIMESTEP,\n",
    ")\n",
    "\n",
    "# Configure P2P components\n",
    "p2p.NUM_DDIM_STEPS = NUM_DDIM_STEPS\n",
    "p2p.tokenizer = ldm_stable.tokenizer\n",
    "p2p.device = \"cuda:0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f64af8d5-a01e-4d5b-9703-351ebf613f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del ldm_stable, reverse_cons_model, forward_cons_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d8a9e1-b524-4105-a8ef-f4d8f0314c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# optimizer = torch.optim.Adam([solver.w_embedding], lr=1e-4)\n",
    "\n",
    "data_files = {\n",
    "    \"test\": \"data/test-*-of-*.parquet\",\n",
    "}\n",
    "dataset = load_dataset(\n",
    "    \"bitmind/MS-COCO\",\n",
    "    data_files=data_files,\n",
    "    split=\"test\",\n",
    "    verification_mode=\"no_checks\",\n",
    ")\n",
    "dataset_sample = dataset.select(\n",
    "    random.sample(range(len(dataset)), 1000)\n",
    ")\n",
    "\n",
    "mse_latent, mse_real = [], []\n",
    "diff_latents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8452d15d-8c4f-4604-97a7-2beaacd02429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dataset\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4eeb90b7-5cd8-4b2b-b3de-24d655e62fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in solver.model.unet.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in reverse_cons_model.unet.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in forward_cons_model.unet.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6333202b-0286-4dc9-8595-e7ff3e7284bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in reverse_cons_model.unet.time_embedding.cond_proj.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d15562a9-4a53-47fe-997f-aafaab8e2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = reverse_cons_model.unet.time_embedding.cond_proj\n",
    "torch.nn.init.normal_(proj.weight, mean=0.0, std=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d7b4b6a-5dca-4dc7-8a9a-193bd125551e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "──────────────────────────────────────\n",
      "UNet2DConditionModel             0\n",
      "UNet2DConditionModel      163.84 K\n",
      "UNet2DConditionModel             0\n",
      "──────────────────────────────────────\n",
      "TOTAL                     163.84 K\n",
      "──────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "163840"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def pretty_count(n):\n",
    "    \"\"\"utility — 1 234 567 → '1.23 M' \"\"\"\n",
    "    if n < 1e3:\n",
    "        return str(n)\n",
    "    elif n < 1e6:\n",
    "        return f\"{n/1e3:,.2f} K\"\n",
    "    elif n < 1e9:\n",
    "        return f\"{n/1e6:,.2f} M\"\n",
    "    else:\n",
    "        return f\"{n/1e9:,.2f} B\"\n",
    "\n",
    "def trainable_parameter_report(*modules):\n",
    "    \"\"\"\n",
    "    Print a small table with #trainable parameters for each module.\n",
    "    Pass any mix of nn.Module objects (or objects that expose .parameters()).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    grand_total = 0\n",
    "    for mod in modules:\n",
    "        n = sum(p.numel() for p in mod.parameters() if p.requires_grad)\n",
    "        rows.append((mod.__class__.__name__, pretty_count(n)))\n",
    "        grand_total += n\n",
    "\n",
    "    width = max(len(name) for name, _ in rows) + 3\n",
    "    print(\"─\" * (width + 15))\n",
    "    for name, cnt in rows:\n",
    "        print(f\"{name:<{width}} {cnt:>10}\")\n",
    "    print(\"─\" * (width + 15))\n",
    "    print(f\"{'TOTAL':<{width}} {pretty_count(grand_total):>10}\")\n",
    "    print(\"─\" * (width + 15))\n",
    "    return grand_total\n",
    "\n",
    "# --- call it ------------------------------------------------\n",
    "trainable_parameter_report(solver.model.unet,               # contains w_embedding\n",
    "                           reverse_cons_model.unet,   # any LoRA or unfrozen layers\n",
    "                           forward_cons_model.unet)   # idem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ce31d22-7816-4b46-827b-649649a6dd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=320, bias=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_cons_model.unet.time_embedding.cond_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d98fe2db-1c9e-4945-8bdd-8cf3f577ade3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163.84"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512 * 320 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "411e5cf6-61c3-4f94-8984-c610bae38236",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    reverse_cons_model.unet.time_embedding.cond_proj.parameters(),\n",
    "    lr=1e-7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5eb013f-248a-4d62-a8e8-2d5e51de4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.p2p import register_attention_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48490e62-7836-4075-9b1e-76d407f34a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_attention_control(solver.model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ccaa0c4-acd8-4c46-bab9-2ab086ec339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_norm(model, norm_type=2):\n",
    "    \"\"\"\n",
    "    Calculates the gradient norm for logging.\n",
    "\n",
    "    Args:\n",
    "        norm_type (float | str | None): the order of the norm.\n",
    "    Returns:\n",
    "        total_norm (float): the calculated norm.\n",
    "    \"\"\"\n",
    "    parameters = model.parameters()\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    total_norm = torch.norm(\n",
    "        torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]),\n",
    "        norm_type,\n",
    "    )\n",
    "    return total_norm.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3bfeefe3-3be5-4a77-9100-3cee91ecc6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch Metrics ---\n",
      "step                : 1\n",
      "batch_start         : 0\n",
      "batch_end           : 1\n",
      "guidance_scale      : 0.0\n",
      "loss                : 0.1994294971227646\n",
      "loss_latent_r_1     : 0.02583257295191288\n",
      "loss_latent_r_2     : 0.06283518671989441\n",
      "loss_latent_r_3     : 0.05859541893005371\n",
      "loss_latent_r_4     : 0.05216630548238754\n",
      "diff_latents_a1     : (0.1251124143600464, 0.02583257295191288)\n",
      "diff_latents_a2     : (0.19543014466762543, 0.06283518671989441)\n",
      "diff_latents_a3     : (0.18874910473823547, 0.05859541893005371)\n",
      "diff_latents_a4     : (0.1778959035873413, 0.05216630548238754)\n",
      "grad_norm           : nan\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 1/1000 [00:01<27:53,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NaN‑probe] after get_noise_pred: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after predicted_origin: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after get_noise_pred: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after predicted_origin: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after get_noise_pred: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after predicted_origin: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after get_noise_pred: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after predicted_origin: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "\n",
      "--- Batch Metrics ---\n",
      "step                : 2\n",
      "batch_start         : 1\n",
      "batch_end           : 2\n",
      "guidance_scale      : 0.0\n",
      "loss                : nan\n",
      "loss_latent_r_1     : nan\n",
      "loss_latent_r_2     : nan\n",
      "loss_latent_r_3     : nan\n",
      "loss_latent_r_4     : nan\n",
      "diff_latents_a1     : (nan, nan)\n",
      "diff_latents_a2     : (nan, nan)\n",
      "diff_latents_a3     : (nan, nan)\n",
      "diff_latents_a4     : (nan, nan)\n",
      "grad_norm           : nan\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 1/1000 [00:03<55:27,  3.33s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "clip_scores, ir_scores = [], []\n",
    "# diff_latents = []\n",
    "mse_latent_log = []\n",
    "mse_real_log = []\n",
    "BATCH_SIZE = 1\n",
    "step_counter = 0\n",
    "\n",
    "for start_idx in tqdm(\n",
    "    range(0, len(dataset_sample), BATCH_SIZE), desc=\"Processing batches\"\n",
    "):\n",
    "    batch = dataset_sample[start_idx : start_idx + BATCH_SIZE]\n",
    "    batch_images = [\n",
    "        img.convert(\"RGB\").resize((512, 512), Image.Resampling.LANCZOS)\n",
    "        for img in batch[\"image\"]\n",
    "    ]\n",
    "    batch_prompts = [s[\"raw\"] for s in batch[\"sentences\"]]\n",
    "    solver.init_prompt(batch_prompts)\n",
    "\n",
    "    # 1) Inversion to get latents from the forward (teacher) direction\n",
    "    # with torch.no_grad():\n",
    "    image_rec1, latents1, latent1 = solver.cons_inversion(\n",
    "        batch_images,\n",
    "        w_embed_dim=512,\n",
    "        guidance_scale=0.0,\n",
    "        seed=0,\n",
    "        use_reverse_model=False,\n",
    "    )\n",
    "    # with torch.amp.autocast(\"cuda\"):\n",
    "    image_rec2, latents2, latent2 = solver.cons_inversion(\n",
    "        batch_images,\n",
    "        w_embed_dim=512,\n",
    "        guidance_scale=0.0,\n",
    "        seed=0,\n",
    "        use_reverse_model=True,\n",
    "    )\n",
    "    \n",
    "    a1 = latents1[1] - latents2[1]\n",
    "    a2 = latents1[2] - latents2[2]\n",
    "    a3 = latents1[3] - latents2[3]\n",
    "    a4 = latents1[4] - latents2[4]\n",
    "\n",
    "    # diff_latents.append((a1.detach().cpu(), a2.detach().cpu(), a3.detach().cpu(), a4.detach().cpu()))\n",
    "\n",
    "    # Use latents from forward pass vs. reverse pass (as an example)\n",
    "    latent_forward1 = latents1[1].detach()  # no grad\n",
    "    latent_reverse1 = latents2[1]  # reverse latent\n",
    "    latent_forward2 = latents1[2].detach()  # no grad\n",
    "    latent_reverse2 = latents2[2]  # reverse latent\n",
    "    latent_forward3 = latents1[3].detach()  # no grad\n",
    "    latent_reverse3 = latents2[3]  # reverse latent\n",
    "    latent_forward4 = latents1[4].detach()  # no grad\n",
    "    latent_reverse4 = latents2[4]  # reverse latent\n",
    "\n",
    "    # 5) Compute MSE loss for training the embedding\n",
    "    loss1 = F.mse_loss(latent_forward1, latent_reverse1)\n",
    "    loss2 = F.mse_loss(latent_forward2, latent_reverse2)\n",
    "    loss3 = F.mse_loss(latent_forward3, latent_reverse3)\n",
    "    loss4 = F.mse_loss(latent_forward4, latent_reverse4)\n",
    "\n",
    "    loss = loss2 + loss3 + loss4 + loss1\n",
    "    # 6) Optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # grad_norm = get_grad_norm(reverse_cons_model.unet.time_embedding.cond_proj)\n",
    "\n",
    "    # torch.nn.utils.clip_grad_norm_(reverse_cons_model.unet.time_embedding.cond_proj.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    # 7) Update step counter and log metrics to wandb\n",
    "    step_counter += 1\n",
    "\n",
    "    # 8) Periodically log sample images to wandb\n",
    "    if step_counter % 1 == 0:\n",
    "        batch_metrics = {\n",
    "            \"step\": step_counter,\n",
    "            \"batch_start\": start_idx,\n",
    "            \"batch_end\": start_idx + BATCH_SIZE,\n",
    "            \"guidance_scale\": 0.0,\n",
    "            \"loss\": loss.item(),\n",
    "            \"loss_latent_r_1\": loss1.item(),\n",
    "            \"loss_latent_r_2\": loss2.item(),\n",
    "            \"loss_latent_r_3\": loss3.item(),\n",
    "            \"loss_latent_r_4\": loss4.item(),\n",
    "\n",
    "            # \"pixel_mse\": pixel_mse,\n",
    "            # \"latent_mse\": latent_mse,\n",
    "            \"diff_latents_a1\": (a1.abs().mean().item(), (a1**2).mean().item()),\n",
    "            \"diff_latents_a2\": (a2.abs().mean().item(), (a2**2).mean().item()),\n",
    "            \"diff_latents_a3\": (a3.abs().mean().item(), (a3**2).mean().item()),\n",
    "            \"diff_latents_a4\": (a4.abs().mean().item(), (a4**2).mean().item()), \n",
    "            \"grad_norm\": grad_norm,\n",
    "        }\n",
    "        # wandb.log(batch_metrics, step=step_counter)\n",
    "        print(\"\\n--- Batch Metrics ---\")\n",
    "        for key, value in batch_metrics.items():\n",
    "            print(f\"{key:20}: {value}\")\n",
    "        print(\"-\" * 40)\n",
    "#         try:\n",
    "#             # with torch.no_grad():\n",
    "#             gen_images_batch, gen_latents_batch, _ = generate_images_batch(\n",
    "#                     solver, reverse_cons_model, batch_prompts, latents2[-1],\n",
    "#             )\n",
    "#             rec_pil = T.ToPILImage()(image_rec2[0].transpose(1, 2, 0))\n",
    "#             gen_pil = T.ToPILImage()(gen_images_batch[0].transpose(1, 2, 0))\n",
    "\n",
    "#             rec_pil.save(os.path.join(\"images\", f\"step_{step_counter}_sample_rec.jpg\"))\n",
    "#             gen_pil.save(os.path.join(\"images\", f\"step_{step_counter}_sample_gen.jpg\"))\n",
    "#             del (\n",
    "#                 gen_images_batch, gen_latents_batch, _\n",
    "#             )\n",
    "#             gc.collect()\n",
    "#             torch.cuda.empty_cache()\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             break\n",
    "\n",
    "    # Cleanup\n",
    "    del (\n",
    "        a1, \n",
    "        a2, \n",
    "        a3, \n",
    "        a4, \n",
    "        image_rec1, \n",
    "        latents1, \n",
    "        latent1, \n",
    "        image_rec2, \n",
    "        latents2, \n",
    "        latent2, \n",
    "        latent_forward1, \n",
    "        latent_reverse1, \n",
    "        latent_forward2, \n",
    "        latent_reverse2, \n",
    "        latent_forward3, \n",
    "        latent_reverse3, \n",
    "        latent_forward4, \n",
    "        latent_reverse4\n",
    "    )\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if step_counter == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de051beb-611a-4eb6-b35b-4e6dd4c09524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NaN‑probe] after get_noise_pred: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after predicted_origin: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after get_noise_pred: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after predicted_origin: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after get_noise_pred: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after predicted_origin: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after get_noise_pred: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n",
      "[NaN‑probe] after predicted_origin: shape=torch.Size([1, 4, 64, 64]) min=nan max=nan\n"
     ]
    }
   ],
   "source": [
    "# image_rec1, latents1, latent1 = solver.cons_inversion(\n",
    "#     batch_images,\n",
    "#     w_embed_dim=512,\n",
    "#     guidance_scale=0.0,\n",
    "#     seed=0,\n",
    "#     use_reverse_model=False,\n",
    "# )\n",
    "# with torch.amp.autocast(\"cuda\"):\n",
    "image_rec2, latents2, latent2 = solver.cons_inversion(\n",
    "    batch_images,\n",
    "    w_embed_dim=512,\n",
    "    guidance_scale=-1.0,\n",
    "    seed=0,\n",
    "    use_reverse_model=True,\n",
    "    use_w_embed=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dc6d762-daf9-41bc-9060-bc724780fb27",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latents1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (l1, l2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mlatents1\u001b[49m, latents2)):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(l2)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaNs in reverse latent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: max=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml2\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latents1' is not defined"
     ]
    }
   ],
   "source": [
    "for i, (l1, l2) in enumerate(zip(latents1, latents2)):\n",
    "    if torch.isnan(l2).any():\n",
    "        print(f\"NaNs in reverse latent {i}: max={l2.abs().max().item():.3e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fd56a-3a4c-4e6e-a158-8bf63882ba06",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13cfef-53ce-45a7-9959-4a9e893d555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.amp.autocast(\"cuda\"):\n",
    "image_rec2, latents2, latent2 = solver.cons_inversion(\n",
    "    batch_images,\n",
    "    w_embed_dim=512,\n",
    "    guidance_scale=-1.0,\n",
    "    seed=0,\n",
    "    use_reverse_model=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0754630-97cd-49ec-afb9-2781efe4e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_images_batch, gen_latents_batch, _ = generate_images_batch(\n",
    "        solver, reverse_cons_model, batch_prompts, latents2[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdecde9-9522-40af-95c4-c1b5273453ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_pil = T.ToPILImage()(image_rec2[0].transpose(1, 2, 0))\n",
    "gen_pil = T.ToPILImage()(gen_images_batch[0].transpose(1, 2, 0))\n",
    "\n",
    "rec_pil.save(os.path.join(\"images\", f\"step_{step_counter}_sample_rec.jpg\"))\n",
    "gen_pil.save(os.path.join(\"images\", f\"step_{step_counter}_sample_gen.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c63a866-82fe-4c8f-9717-74da39c76784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del (\n",
    "#     image_rec2, latents2, latent2, gen_images_batch, gen_latents_batch, _\n",
    "# )\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google Colab Analog 2024 (PyTorch 2.5.1 + TensorFlow 2.18) [python-google_colab_gpu_2024]",
   "language": "python",
   "name": "conda-env-python-google_colab_gpu_2024-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
