{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782b8d62-b895-4d97-800f-d9e2615358cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 12:41:36.677714: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744882896.707354  267900 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744882896.716426  267900 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from diffusers import LCMScheduler, DDPMScheduler, StableDiffusionPipeline\n",
    "# import ImageReward as RM\n",
    "# from torchmetrics.functional.multimodal import clip_score\n",
    "# from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision.transforms.functional import to_tensor, resize\n",
    "from diffusers.models.attention_processor import AttnProcessor2_0\n",
    "\n",
    "from utils.loading import load_models\n",
    "from utils import p2p, generation, inversion\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "NUM_REVERSE_CONS_STEPS = 4\n",
    "REVERSE_TIMESTEPS = [259, 519, 779, 999]\n",
    "NUM_FORWARD_CONS_STEPS = 4\n",
    "FORWARD_TIMESTEPS = [19, 259, 519, 779]\n",
    "NUM_DDIM_STEPS = 50\n",
    "START_TIMESTEP = 19\n",
    "\n",
    "def generate_images_batch(solver, reverse_cons_model, prompts, latent):\n",
    "    images = []\n",
    "    generator = torch.Generator(device=\"cuda:0\").manual_seed(42)\n",
    "    controller = p2p.AttentionStore()\n",
    "    images, gen_latent, latents = generation.runner(\n",
    "        guidance_scale=0.0,\n",
    "        tau1=1.0,\n",
    "        tau2=1.0,\n",
    "        is_cons_forward=True,\n",
    "        model=reverse_cons_model,\n",
    "        dynamic_guidance=False,\n",
    "        w_embed_dim=512,\n",
    "        start_time=50,\n",
    "        solver=solver,\n",
    "        prompt=prompts,\n",
    "        controller=controller,\n",
    "        generator=generator,\n",
    "        latent=latent,\n",
    "        return_type=\"image\",\n",
    "        # num_inference_steps=50,\n",
    "    )\n",
    "    return images, gen_latent, latents\n",
    "\n",
    "\n",
    "def invert_images_batch(solver, prompts, images, guidance_scale, use_reverse_model=False):\n",
    "    (image_gt, image_rec), latents, uncond_embeddings, latent_orig = inversion.invert(\n",
    "        is_cons_inversion=True,\n",
    "        # do_npi=False,\n",
    "        # do_nti=True,\n",
    "        w_embed_dim=512,\n",
    "        stop_step=50,  # from [0, NUM_DDIM_STEPS]\n",
    "        inv_guidance_scale=guidance_scale,\n",
    "        dynamic_guidance=False,\n",
    "        tau1=0.0,\n",
    "        tau2=0.0,\n",
    "        solver=solver,\n",
    "        images=images,\n",
    "        prompt=prompts,\n",
    "        # num_inner_steps=10,\n",
    "        # early_stop_epsilon=1e-5,\n",
    "        seed=42,\n",
    "        use_reverse_model=use_reverse_model\n",
    "    )\n",
    "\n",
    "    return image_gt, image_rec, latents[-1], latents, latent_orig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd1d3ce-66de-4eaf-a150-f8998eb89e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed551c32b6f438dbf04dc42d14f6072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/python/envs/google_colab_gpu_2024/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:221: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
      "  \"_class_name\": \"DDIMScheduler\",\n",
      "  \"_diffusers_version\": \"0.31.0\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"scaled_linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": false,\n",
      "  \"clip_sample_range\": 1.0,\n",
      "  \"dynamic_thresholding_ratio\": 0.995,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"rescale_betas_zero_snr\": false,\n",
      "  \"sample_max_value\": 1.0,\n",
      "  \"set_alpha_to_one\": false,\n",
      "  \"steps_offset\": 0,\n",
      "  \"thresholding\": false,\n",
      "  \"timestep_spacing\": \"leading\",\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward CD is initialized with guidance embedding, dim 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of UNet2DConditionModel were not initialized from the model checkpoint at sd-legacy/stable-diffusion-v1-5 and are newly initialized: ['time_embedding.cond_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded model is loading from checkpoints/sd15_cfg_distill.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaytkhadzhaev/accelerating diffusion/inversion_eval/invertible-cd/utils/loading.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unet.load_state_dict(torch.load(teacher_checkpoint))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse CD is loading from checkpoints/iCD-SD1.5_259_519_779_999.safetensors\n",
      "Forward CD is loading from checkpoints/iCD-SD1.5_19_259_519_779.safetensors\n",
      "Endpoints reverse CTM: tensor([999, 779, 519, 259]), tensor([779, 519, 259,   0])\n",
      "Endpoints forward CTM: tensor([ 19, 259, 519, 779]), tensor([259, 519, 779, 999])\n"
     ]
    }
   ],
   "source": [
    "ldm_stable, reverse_cons_model, forward_cons_model = load_models(\n",
    "    model_id=\"sd-legacy/stable-diffusion-v1-5\",\n",
    "    device=\"cuda:0\",\n",
    "    forward_checkpoint=\"checkpoints/iCD-SD1.5_19_259_519_779.safetensors\",\n",
    "    reverse_checkpoint=\"checkpoints/iCD-SD1.5_259_519_779_999.safetensors\",\n",
    "    r=64,\n",
    "    w_embed_dim=512,\n",
    "    teacher_checkpoint=\"checkpoints/sd15_cfg_distill.pt\",\n",
    "    dtype=\"fp16\",\n",
    ")\n",
    "ldm_stable.unet.set_attn_processor(AttnProcessor2_0())\n",
    "reverse_cons_model.unet.set_attn_processor(AttnProcessor2_0())\n",
    "forward_cons_model.unet.set_attn_processor(AttnProcessor2_0())\n",
    "\n",
    "ldm_stable.set_progress_bar_config(disable=True)\n",
    "reverse_cons_model.set_progress_bar_config(disable=True)\n",
    "forward_cons_model.set_progress_bar_config(disable=True)\n",
    "\n",
    "ldm_stable.safety_checker = None\n",
    "reverse_cons_model.safety_checker = None\n",
    "forward_cons_model.safety_checker = None\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"scheduler\",\n",
    ")\n",
    "\n",
    "solver = generation.Generator(\n",
    "    model=ldm_stable,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    n_steps=NUM_DDIM_STEPS,\n",
    "    forward_cons_model=forward_cons_model,\n",
    "    forward_timesteps=FORWARD_TIMESTEPS,\n",
    "    reverse_cons_model=reverse_cons_model,\n",
    "    reverse_timesteps=REVERSE_TIMESTEPS,\n",
    "    num_endpoints=NUM_REVERSE_CONS_STEPS,\n",
    "    num_forward_endpoints=NUM_FORWARD_CONS_STEPS,\n",
    "    max_forward_timestep_index=49,\n",
    "    start_timestep=START_TIMESTEP,\n",
    ")\n",
    "\n",
    "# Configure P2P components\n",
    "p2p.NUM_DDIM_STEPS = NUM_DDIM_STEPS\n",
    "p2p.tokenizer = ldm_stable.tokenizer\n",
    "p2p.device = \"cuda:0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d8a9e1-b524-4105-a8ef-f4d8f0314c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# optimizer = torch.optim.Adam([solver.w_embedding], lr=1e-4)\n",
    "\n",
    "data_files = {\n",
    "    \"test\": \"data/test-*-of-*.parquet\",\n",
    "}\n",
    "dataset = load_dataset(\n",
    "    \"bitmind/MS-COCO\",\n",
    "    data_files=data_files,\n",
    "    split=\"test\",\n",
    "    verification_mode=\"no_checks\",\n",
    ")\n",
    "dataset_sample = dataset.select(\n",
    "    random.sample(range(len(dataset)), 1000)\n",
    ")\n",
    "\n",
    "mse_latent, mse_real = [], []\n",
    "diff_latents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8452d15d-8c4f-4604-97a7-2beaacd02429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dataset\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d61b2f63-a788-41eb-a37b-e1da81cdf1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.w_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eeb90b7-5cd8-4b2b-b3de-24d655e62fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in solver.model.unet.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in reverse_cons_model.unet.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in forward_cons_model.unet.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6333202b-0286-4dc9-8595-e7ff3e7284bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.w_embedding.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d7b4b6a-5dca-4dc7-8a9a-193bd125551e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "──────────────────────────────────────\n",
      "UNet2DConditionModel             0\n",
      "UNet2DConditionModel             0\n",
      "UNet2DConditionModel             0\n",
      "──────────────────────────────────────\n",
      "TOTAL                            0\n",
      "──────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def pretty_count(n):\n",
    "    \"\"\"utility — 1 234 567 → '1.23 M' \"\"\"\n",
    "    if n < 1e3:\n",
    "        return str(n)\n",
    "    elif n < 1e6:\n",
    "        return f\"{n/1e3:,.2f} K\"\n",
    "    elif n < 1e9:\n",
    "        return f\"{n/1e6:,.2f} M\"\n",
    "    else:\n",
    "        return f\"{n/1e9:,.2f} B\"\n",
    "\n",
    "def trainable_parameter_report(*modules):\n",
    "    \"\"\"\n",
    "    Print a small table with #trainable parameters for each module.\n",
    "    Pass any mix of nn.Module objects (or objects that expose .parameters()).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    grand_total = 0\n",
    "    for mod in modules:\n",
    "        n = sum(p.numel() for p in mod.parameters() if p.requires_grad)\n",
    "        rows.append((mod.__class__.__name__, pretty_count(n)))\n",
    "        grand_total += n\n",
    "\n",
    "    width = max(len(name) for name, _ in rows) + 3\n",
    "    print(\"─\" * (width + 15))\n",
    "    for name, cnt in rows:\n",
    "        print(f\"{name:<{width}} {cnt:>10}\")\n",
    "    print(\"─\" * (width + 15))\n",
    "    print(f\"{'TOTAL':<{width}} {pretty_count(grand_total):>10}\")\n",
    "    print(\"─\" * (width + 15))\n",
    "    return grand_total\n",
    "\n",
    "# --- call it ------------------------------------------------\n",
    "trainable_parameter_report(solver.model.unet,               # contains w_embedding\n",
    "                           reverse_cons_model.unet,   # any LoRA or unfrozen layers\n",
    "                           forward_cons_model.unet)   # idem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce31d22-7816-4b46-827b-649649a6dd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "411e5cf6-61c3-4f94-8984-c610bae38236",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([solver.w_embedding], lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5eb013f-248a-4d62-a8e8-2d5e51de4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.p2p import register_attention_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48490e62-7836-4075-9b1e-76d407f34a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_attention_control(solver.model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bfeefe3-3be5-4a77-9100-3cee91ecc6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  10%|▉         | 49/500 [02:57<26:45,  3.56s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch Metrics ---\n",
      "step                : 50\n",
      "batch_start         : 98\n",
      "batch_end           : 100\n",
      "guidance_scale      : -1.0\n",
      "loss                : 0.16998666524887085\n",
      "loss_latent_r_1     : 7.450540806530626e-07\n",
      "loss_latent_r_2     : 0.0281115360558033\n",
      "loss_latent_r_3     : 0.06867031753063202\n",
      "loss_latent_r_4     : 0.07320407032966614\n",
      "diff_latents_a1     : 0.00017747882520779967\n",
      "diff_latents_a2     : 0.00236098887398839\n",
      "diff_latents_a3     : 0.00016356274136342108\n",
      "diff_latents_a4     : 0.0024823672138154507\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|█▉        | 99/500 [05:28<18:56,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch Metrics ---\n",
      "step                : 100\n",
      "batch_start         : 198\n",
      "batch_end           : 200\n",
      "guidance_scale      : -1.0\n",
      "loss                : 0.16600432991981506\n",
      "loss_latent_r_1     : 5.879029458810692e-07\n",
      "loss_latent_r_2     : 0.02899692766368389\n",
      "loss_latent_r_3     : 0.06601914763450623\n",
      "loss_latent_r_4     : 0.07098766416311264\n",
      "diff_latents_a1     : -0.0003876169503200799\n",
      "diff_latents_a2     : 0.0018087518401443958\n",
      "diff_latents_a3     : -0.0006995809962972999\n",
      "diff_latents_a4     : 0.0007289305794984102\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  30%|██▉       | 149/500 [08:06<20:42,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch Metrics ---\n",
      "step                : 150\n",
      "batch_start         : 298\n",
      "batch_end           : 300\n",
      "guidance_scale      : -1.0\n",
      "loss                : 0.16854453086853027\n",
      "loss_latent_r_1     : 9.478814035901451e-07\n",
      "loss_latent_r_2     : 0.026401493698358536\n",
      "loss_latent_r_3     : 0.0669550895690918\n",
      "loss_latent_r_4     : 0.07518700510263443\n",
      "diff_latents_a1     : 0.00010483618825674057\n",
      "diff_latents_a2     : 0.001457225065678358\n",
      "diff_latents_a3     : -0.0009447133634239435\n",
      "diff_latents_a4     : 0.001242521102540195\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|███▉      | 199/500 [11:03<18:21,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch Metrics ---\n",
      "step                : 200\n",
      "batch_start         : 398\n",
      "batch_end           : 400\n",
      "guidance_scale      : -1.0\n",
      "loss                : 0.1383953094482422\n",
      "loss_latent_r_1     : 7.775688573019579e-07\n",
      "loss_latent_r_2     : 0.02527574822306633\n",
      "loss_latent_r_3     : 0.052658144384622574\n",
      "loss_latent_r_4     : 0.0604606494307518\n",
      "diff_latents_a1     : 0.0001659216359257698\n",
      "diff_latents_a2     : 0.002881997497752309\n",
      "diff_latents_a3     : 0.002658013254404068\n",
      "diff_latents_a4     : 0.004113596398383379\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  50%|████▉     | 249/500 [13:43<13:19,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch Metrics ---\n",
      "step                : 250\n",
      "batch_start         : 498\n",
      "batch_end           : 500\n",
      "guidance_scale      : -1.0\n",
      "loss                : 0.131059929728508\n",
      "loss_latent_r_1     : 1.227371967615909e-06\n",
      "loss_latent_r_2     : 0.022724082693457603\n",
      "loss_latent_r_3     : 0.05129890888929367\n",
      "loss_latent_r_4     : 0.05703571438789368\n",
      "diff_latents_a1     : 0.0007219183025881648\n",
      "diff_latents_a2     : 0.0015598334139212966\n",
      "diff_latents_a3     : 0.0006340585532598197\n",
      "diff_latents_a4     : 0.004471641965210438\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  60%|█████▉    | 299/500 [16:25<10:00,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch Metrics ---\n",
      "step                : 300\n",
      "batch_start         : 598\n",
      "batch_end           : 600\n",
      "guidance_scale      : -1.0\n",
      "loss                : 0.16027449071407318\n",
      "loss_latent_r_1     : 5.162271463632351e-07\n",
      "loss_latent_r_2     : 0.02746780775487423\n",
      "loss_latent_r_3     : 0.06329695880413055\n",
      "loss_latent_r_4     : 0.06950920820236206\n",
      "diff_latents_a1     : -0.000737191759981215\n",
      "diff_latents_a2     : 0.0014383954694494605\n",
      "diff_latents_a3     : 0.0009117266163229942\n",
      "diff_latents_a4     : 0.0026650296058505774\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  70%|██████▉   | 349/500 [19:12<08:17,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch Metrics ---\n",
      "step                : 350\n",
      "batch_start         : 698\n",
      "batch_end           : 700\n",
      "guidance_scale      : -1.0\n",
      "loss                : 0.12836427986621857\n",
      "loss_latent_r_1     : 6.714975029353809e-07\n",
      "loss_latent_r_2     : 0.026609841734170914\n",
      "loss_latent_r_3     : 0.04867025464773178\n",
      "loss_latent_r_4     : 0.05308350920677185\n",
      "diff_latents_a1     : 0.0007436739979311824\n",
      "diff_latents_a2     : 0.002547305077314377\n",
      "diff_latents_a3     : -0.0006382087012752891\n",
      "diff_latents_a4     : 0.000897866440936923\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  80%|███████▉  | 399/500 [22:03<05:24,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch Metrics ---\n",
      "step                : 400\n",
      "batch_start         : 798\n",
      "batch_end           : 800\n",
      "guidance_scale      : -1.0\n",
      "loss                : 0.1448948234319687\n",
      "loss_latent_r_1     : 8.483624469590723e-07\n",
      "loss_latent_r_2     : 0.024445462971925735\n",
      "loss_latent_r_3     : 0.055543653666973114\n",
      "loss_latent_r_4     : 0.06490486115217209\n",
      "diff_latents_a1     : -6.769521860405803e-06\n",
      "diff_latents_a2     : 0.0023037190549075603\n",
      "diff_latents_a3     : 0.0017033027252182364\n",
      "diff_latents_a4     : 0.0016092198202386498\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  90%|████████▉ | 449/500 [24:42<02:46,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch Metrics ---\n",
      "step                : 450\n",
      "batch_start         : 898\n",
      "batch_end           : 900\n",
      "guidance_scale      : -1.0\n",
      "loss                : 0.15207573771476746\n",
      "loss_latent_r_1     : 1.2298562523938017e-06\n",
      "loss_latent_r_2     : 0.025367118418216705\n",
      "loss_latent_r_3     : 0.061676423996686935\n",
      "loss_latent_r_4     : 0.06503095477819443\n",
      "diff_latents_a1     : 0.00040045345667749643\n",
      "diff_latents_a2     : 0.0028753923252224922\n",
      "diff_latents_a3     : 0.0006061262101866305\n",
      "diff_latents_a4     : 0.0010991034796461463\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|█████████▉| 499/500 [27:15<00:03,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch Metrics ---\n",
      "step                : 500\n",
      "batch_start         : 998\n",
      "batch_end           : 1000\n",
      "guidance_scale      : -1.0\n",
      "loss                : 0.13874554634094238\n",
      "loss_latent_r_1     : 9.518190040580521e-07\n",
      "loss_latent_r_2     : 0.024412106722593307\n",
      "loss_latent_r_3     : 0.052048925310373306\n",
      "loss_latent_r_4     : 0.06228356808423996\n",
      "diff_latents_a1     : 5.621431046165526e-05\n",
      "diff_latents_a2     : 0.0018030592473223805\n",
      "diff_latents_a3     : 0.002017838880419731\n",
      "diff_latents_a4     : 0.0018467081245034933\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 500/500 [27:18<00:00,  3.28s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "clip_scores, ir_scores = [], []\n",
    "# diff_latents = []\n",
    "mse_latent_log = []\n",
    "mse_real_log = []\n",
    "BATCH_SIZE = 2\n",
    "step_counter = 0\n",
    "\n",
    "for start_idx in tqdm(\n",
    "    range(0, len(dataset_sample), BATCH_SIZE), desc=\"Processing batches\"\n",
    "):\n",
    "    batch = dataset_sample[start_idx : start_idx + BATCH_SIZE]\n",
    "    batch_images = [\n",
    "        img.convert(\"RGB\").resize((512, 512), Image.Resampling.LANCZOS)\n",
    "        for img in batch[\"image\"]\n",
    "    ]\n",
    "    batch_prompts = [s[\"raw\"] for s in batch[\"sentences\"]]\n",
    "    solver.init_prompt(batch_prompts)\n",
    "\n",
    "    # 1) Inversion to get latents from the forward (teacher) direction\n",
    "    # with torch.no_grad():\n",
    "    image_rec1, latents1, latent1 = solver.cons_inversion(\n",
    "        batch_images,\n",
    "        w_embed_dim=0,\n",
    "        guidance_scale=0.0,\n",
    "        seed=0,\n",
    "        use_reverse_model=False,\n",
    "    )\n",
    "    with torch.amp.autocast(\"cuda\"):\n",
    "        image_rec2, latents2, latent2 = solver.cons_inversion(\n",
    "            batch_images,\n",
    "            w_embed_dim=512,\n",
    "            guidance_scale=-1.0,\n",
    "            seed=0,\n",
    "            use_reverse_model=True,\n",
    "            use_w_embed=True,\n",
    "        )\n",
    "    \n",
    "    a1 = latents1[1] - latents2[1]\n",
    "    a2 = latents1[2] - latents2[2]\n",
    "    a3 = latents1[3] - latents2[3]\n",
    "    a4 = latents1[4] - latents2[4]\n",
    "\n",
    "    # diff_latents.append((a1.detach().cpu(), a2.detach().cpu(), a3.detach().cpu(), a4.detach().cpu()))\n",
    "\n",
    "    # Use latents from forward pass vs. reverse pass (as an example)\n",
    "    latent_forward1 = latents1[0].detach()  # no grad\n",
    "    latent_reverse1 = latents2[0]  # reverse latent\n",
    "    latent_forward2 = latents1[1].detach()  # no grad\n",
    "    latent_reverse2 = latents2[1]  # reverse latent\n",
    "    latent_forward3 = latents1[2].detach()  # no grad\n",
    "    latent_reverse3 = latents2[2]  # reverse latent\n",
    "    latent_forward4 = latents1[3].detach()  # no grad\n",
    "    latent_reverse4 = latents2[3]  # reverse latent\n",
    "\n",
    "    # 5) Compute MSE loss for training the embedding\n",
    "    loss1 = F.mse_loss(latent_forward1, latent_reverse1)\n",
    "    loss2 = F.mse_loss(latent_forward2, latent_reverse2)\n",
    "    loss3 = F.mse_loss(latent_forward3, latent_reverse3)\n",
    "    loss4 = F.mse_loss(latent_forward4, latent_reverse4)\n",
    "\n",
    "    loss = loss1 + loss2 + loss3 + loss4\n",
    "    # 6) Optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 7) Update step counter and log metrics to wandb\n",
    "    step_counter += 1\n",
    "\n",
    "    # 8) Periodically log sample images to wandb\n",
    "    if step_counter % 50 == 0:\n",
    "        batch_metrics = {\n",
    "            \"step\": step_counter,\n",
    "            \"batch_start\": start_idx,\n",
    "            \"batch_end\": start_idx + BATCH_SIZE,\n",
    "            \"guidance_scale\": -1.0,\n",
    "            \"loss\": loss.item(),\n",
    "            \"loss_latent_r_1\": loss1.item(),\n",
    "            \"loss_latent_r_2\": loss2.item(),\n",
    "            \"loss_latent_r_3\": loss3.item(),\n",
    "            \"loss_latent_r_4\": loss4.item(),\n",
    "\n",
    "            # \"pixel_mse\": pixel_mse,\n",
    "            # \"latent_mse\": latent_mse,\n",
    "            \"diff_latents_a1\": a1.mean().item(),\n",
    "            \"diff_latents_a2\": a2.mean().item(),\n",
    "            \"diff_latents_a3\": a3.mean().item(),\n",
    "            \"diff_latents_a4\": a4.mean().item(),\n",
    "        }\n",
    "        # wandb.log(batch_metrics, step=step_counter)\n",
    "        print(\"\\n--- Batch Metrics ---\")\n",
    "        for key, value in batch_metrics.items():\n",
    "            print(f\"{key:20}: {value}\")\n",
    "        print(\"-\" * 40)\n",
    "#         try:\n",
    "#             # with torch.no_grad():\n",
    "#             gen_images_batch, gen_latents_batch, _ = generate_images_batch(\n",
    "#                     solver, reverse_cons_model, batch_prompts, latents2[-1],\n",
    "#             )\n",
    "#             rec_pil = T.ToPILImage()(image_rec2[0].transpose(1, 2, 0))\n",
    "#             gen_pil = T.ToPILImage()(gen_images_batch[0].transpose(1, 2, 0))\n",
    "\n",
    "#             rec_pil.save(os.path.join(\"images\", f\"step_{step_counter}_sample_rec.jpg\"))\n",
    "#             gen_pil.save(os.path.join(\"images\", f\"step_{step_counter}_sample_gen.jpg\"))\n",
    "#             del (\n",
    "#                 gen_images_batch, gen_latents_batch, _\n",
    "#             )\n",
    "#             gc.collect()\n",
    "#             torch.cuda.empty_cache()\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             break\n",
    "\n",
    "    # Cleanup\n",
    "    del (\n",
    "        a1, \n",
    "        a2, \n",
    "        a3, \n",
    "        a4, \n",
    "        image_rec1, \n",
    "        latents1, \n",
    "        latent1, \n",
    "        image_rec2, \n",
    "        latents2, \n",
    "        latent2, \n",
    "        latent_forward1, \n",
    "        latent_reverse1, \n",
    "        latent_forward2, \n",
    "        latent_reverse2, \n",
    "        latent_forward3, \n",
    "        latent_reverse3, \n",
    "        latent_forward4, \n",
    "        latent_reverse4\n",
    "    )\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fd56a-3a4c-4e6e-a158-8bf63882ba06",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f13cfef-53ce-45a7-9959-4a9e893d555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.amp.autocast(\"cuda\"):\n",
    "    image_rec2, latents2, latent2 = solver.cons_inversion(\n",
    "        batch_images,\n",
    "        w_embed_dim=512,\n",
    "        guidance_scale=-1.0,\n",
    "        seed=0,\n",
    "        use_reverse_model=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0754630-97cd-49ec-afb9-2781efe4e0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaytkhadzhaev/accelerating diffusion/inversion_eval/invertible-cd/utils/generation.py:710: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  batch_size, model.unet.in_channels, height // 8, width // 8\n"
     ]
    }
   ],
   "source": [
    "gen_images_batch, gen_latents_batch, _ = generate_images_batch(\n",
    "        solver, reverse_cons_model, batch_prompts, latents2[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abdecde9-9522-40af-95c4-c1b5273453ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_pil = T.ToPILImage()(image_rec2[0].transpose(1, 2, 0))\n",
    "gen_pil = T.ToPILImage()(gen_images_batch[0].transpose(1, 2, 0))\n",
    "\n",
    "rec_pil.save(os.path.join(\"images\", f\"step_{step_counter}_sample_rec.jpg\"))\n",
    "gen_pil.save(os.path.join(\"images\", f\"step_{step_counter}_sample_gen.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c63a866-82fe-4c8f-9717-74da39c76784",
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    image_rec2, latents2, latent2, gen_images_batch, gen_latents_batch, _\n",
    ")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc9b20be-9319-451c-b27a-1d7624c1c503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.3167e+00,  1.0265e+00,  5.8795e-01,  4.3646e-01,  1.0810e+00,\n",
       "         -3.9021e-01,  9.2430e-01,  3.5386e-01,  1.8032e-01,  1.4343e+00,\n",
       "         -8.3853e-01, -9.2224e-01, -1.3401e+00, -1.3697e+00, -2.0591e+00,\n",
       "         -2.5465e-01, -1.7250e-01,  6.6381e-02, -4.0814e-01,  2.5360e-01,\n",
       "          9.9003e-02, -1.9166e-01,  2.3489e+00,  4.2554e-01, -1.7141e+00,\n",
       "          6.9852e-01, -5.3022e-01,  1.6999e+00, -7.4754e-01, -1.0702e-01,\n",
       "          2.4658e-01,  9.6704e-01, -1.4666e+00, -8.1104e-01,  5.7378e-01,\n",
       "          4.7321e-01, -7.2670e-04,  1.4495e+00,  1.0640e+00,  1.1645e+00,\n",
       "         -3.4560e-01,  2.0137e+00, -5.1223e-01,  2.0068e+00, -7.0276e-01,\n",
       "         -5.1158e-01, -1.7230e+00,  2.3604e-01,  1.1305e+00, -6.5812e-01,\n",
       "         -3.6016e-01, -1.2838e+00,  1.2412e+00,  1.2128e+00, -3.2235e-01,\n",
       "          7.5720e-01, -3.0015e-01, -1.7045e+00, -4.4538e-01, -7.6546e-01,\n",
       "         -1.9902e-01,  4.5017e-01,  9.9657e-01, -2.0427e+00, -1.1009e+00,\n",
       "          1.0661e+00,  9.3520e-03,  7.0294e-01, -1.1049e+00, -2.0169e-01,\n",
       "          5.4058e-01, -9.6113e-01,  1.8910e-01,  7.8865e-01, -5.0402e-01,\n",
       "          1.2215e-01, -6.9939e-01, -5.4396e-01, -9.2773e-01,  7.6355e-01,\n",
       "          2.1268e-01,  8.1396e-02,  1.4977e-01, -1.3293e-01, -8.7852e-01,\n",
       "         -2.9078e-01, -2.8744e-02, -9.9772e-01,  7.0287e-02, -1.0589e+00,\n",
       "         -4.6848e-01, -1.3639e-01,  3.3814e-01,  1.0525e+00,  1.6866e-01,\n",
       "         -1.8445e-01,  9.1177e-01, -1.4928e+00,  3.4214e-01, -2.7892e-01,\n",
       "         -5.7964e-02, -1.4583e+00,  2.3530e+00,  5.0072e-01,  4.0590e-01,\n",
       "          9.9145e-01, -4.8637e-01, -6.8836e-01, -1.6186e+00, -3.5623e-01,\n",
       "          1.3356e+00, -1.1083e+00,  9.8398e-01, -6.0979e-01, -1.1082e+00,\n",
       "         -1.0996e+00, -3.7036e-01,  7.3096e-01, -4.7630e-01, -1.1386e-01,\n",
       "          2.0456e+00, -5.9557e-01, -1.1869e+00, -3.6183e-01,  8.2726e-01,\n",
       "         -6.7269e-01, -9.8287e-01, -2.4079e-01, -1.1759e+00,  3.8474e-01,\n",
       "         -2.6701e-01, -4.7881e-01,  4.8337e-01, -8.9088e-01,  1.4053e-02,\n",
       "         -1.0788e+00,  8.9692e-02,  3.1853e-01,  5.8762e-01, -1.0730e+00,\n",
       "         -1.4195e+00,  8.4507e-01,  8.0482e-01, -9.3523e-01, -2.0672e-02,\n",
       "          1.4584e+00, -1.9244e-01, -2.7389e-01, -2.5180e+00,  7.1957e-01,\n",
       "          7.2222e-01, -3.5989e-01,  9.3338e-01,  4.6949e-01,  7.1966e-01,\n",
       "          1.0588e+00,  8.3749e-01, -2.2543e-01, -8.2837e-01, -5.4768e-01,\n",
       "         -7.4032e-01,  5.8067e-01,  1.0898e+00,  7.2889e-01, -1.2046e+00,\n",
       "          4.8285e-01, -6.3498e-01,  9.1992e-01,  1.5405e-01,  1.7490e-01,\n",
       "         -1.1122e+00,  1.5139e+00, -7.9164e-02,  8.7157e-01, -6.2508e-02,\n",
       "         -5.6441e-01, -5.4181e-01, -1.5865e-01, -5.5483e-02, -4.5779e-01,\n",
       "          2.7020e-01,  4.2548e-01,  1.1612e+00, -5.5309e-02, -2.7148e-01,\n",
       "          1.4017e+00, -2.0935e-01,  1.8266e+00,  4.5240e-01,  2.3453e-01,\n",
       "         -1.0418e-01,  1.4105e+00,  4.6092e-03, -1.3829e+00,  1.1049e+00,\n",
       "          1.9029e-01, -5.0753e-02, -1.0163e+00,  7.0137e-01, -1.5202e+00,\n",
       "         -3.7304e-01,  1.5842e+00,  5.6054e-01,  1.4679e+00,  3.1404e-01,\n",
       "         -4.6839e-01,  8.0281e-01, -1.3577e-01, -1.3377e+00, -8.9241e-01,\n",
       "         -7.1974e-01,  5.7578e-01,  7.8003e-02,  4.1793e-01, -1.1092e+00,\n",
       "         -4.3705e-03,  3.3718e-01, -7.6863e-01, -1.2525e+00, -5.9873e-01,\n",
       "          1.8624e-01, -2.0798e+00, -1.2818e-01, -3.5289e-01,  1.7176e+00,\n",
       "          3.8926e-01,  1.1295e+00,  6.0414e-01,  1.3746e+00,  1.3180e+00,\n",
       "         -1.1943e+00, -3.1744e-01,  5.1545e-03,  5.9835e-01, -2.0803e-02,\n",
       "         -9.0315e-01, -8.3785e-01,  2.3641e-01,  2.0426e-01,  7.9380e-01,\n",
       "         -6.1027e-01, -8.4665e-01,  2.4851e-01,  8.8737e-01, -5.5464e-01,\n",
       "          1.2314e+00, -2.6257e+00, -1.4121e+00, -2.3862e+00, -3.3274e-01,\n",
       "          9.4390e-01,  8.1579e-01, -3.2569e-01, -6.1730e-01, -9.5465e-01,\n",
       "          9.0085e-01,  4.0628e-01,  1.1050e+00, -1.2407e+00,  1.5976e+00,\n",
       "          6.9870e-01, -7.6045e-01, -1.7040e+00,  1.1890e+00,  7.4115e-01,\n",
       "          2.7307e-01, -2.6184e+00,  8.4814e-01,  2.0083e+00,  1.0840e+00,\n",
       "          9.8305e-01,  8.6642e-01, -1.4343e+00, -1.0794e+00,  1.1137e+00,\n",
       "         -8.3587e-01, -1.1207e+00,  7.1428e-01,  5.1642e-01, -3.0129e-01,\n",
       "          2.0540e-01,  4.1878e-01,  1.0216e+00,  1.1134e-01,  1.7158e+00,\n",
       "         -5.5016e-02,  8.8076e-01,  3.6060e-01,  1.3776e+00, -4.0711e-01,\n",
       "          3.4821e+00,  1.1587e+00,  6.3738e-01, -3.3735e-02,  2.6077e+00,\n",
       "          9.5058e-01, -2.0980e+00,  1.0795e+00, -1.2093e+00,  2.7357e-01,\n",
       "          2.0433e-01,  1.7792e+00, -5.3312e-01, -8.6021e-01, -1.9174e-01,\n",
       "          1.5806e+00,  6.0030e-02,  3.8483e-01, -2.1847e-01,  9.4853e-01,\n",
       "          1.0227e+00,  2.9430e-01,  5.5307e-01, -1.4739e+00, -1.8599e+00,\n",
       "          1.8399e+00,  1.1475e+00,  5.7899e-02,  1.5540e-01,  8.1230e-01,\n",
       "          1.1325e-01, -1.0822e+00,  1.3866e+00, -1.0086e+00,  1.2028e-01,\n",
       "          2.2877e-01, -2.4461e+00, -1.5769e+00, -7.1653e-01, -2.7449e+00,\n",
       "          2.6660e-01,  1.0999e+00,  1.1466e+00,  5.5943e-01,  5.7571e-01,\n",
       "          1.7427e+00,  1.5026e+00, -1.4962e+00,  1.0717e+00, -7.0708e-01,\n",
       "         -3.9762e-01,  1.2731e+00, -8.7895e-01, -1.0370e+00,  6.5212e-02,\n",
       "         -1.5896e+00,  2.1009e-01, -3.7422e-01, -2.9164e-01, -1.1788e+00,\n",
       "          1.4356e+00, -8.3297e-01,  1.0128e+00,  6.2660e-01, -2.6499e-01,\n",
       "          4.8904e-01,  2.1802e+00, -4.7900e-01, -1.4062e+00, -1.6169e+00,\n",
       "         -6.9123e-01,  4.0925e-01, -1.3149e-01, -1.0051e-01,  3.9001e-01,\n",
       "         -3.3558e-01,  7.2672e-02,  1.1459e-01,  2.3045e-01, -2.3278e-02,\n",
       "          8.9477e-01, -5.5314e-01, -3.1273e-01,  1.2510e+00,  3.6593e-01,\n",
       "          1.6794e+00, -1.0016e-01, -1.6180e-01,  8.4876e-01, -1.5356e+00,\n",
       "          2.8824e+00,  1.0697e-01,  8.3491e-01,  2.5655e-01, -2.3962e+00,\n",
       "         -1.2583e+00,  7.1775e-01, -2.2796e-01,  2.1986e+00,  1.0164e-01,\n",
       "         -9.8834e-01, -1.0370e+00,  1.1663e+00, -5.3084e-01,  3.6476e-01,\n",
       "          5.0175e-01,  1.8475e+00, -1.0096e-01,  7.4281e-01,  1.0138e+00,\n",
       "         -8.1858e-01,  6.3430e-01,  3.6936e-01,  2.1772e+00, -9.6715e-01,\n",
       "         -1.3755e-02, -1.5954e+00, -2.6151e-01, -1.3890e+00,  5.9362e-02,\n",
       "         -4.5750e-01,  1.8870e+00,  1.8221e+00,  1.2212e+00, -4.8623e-01,\n",
       "         -2.6385e-02, -2.4921e+00, -1.1356e+00, -2.0301e-01, -5.4073e-01,\n",
       "          4.2061e-01,  1.3348e+00, -4.7037e-01,  1.1817e+00,  2.8091e-01,\n",
       "          4.3172e-02, -8.1584e-01, -5.8019e-01, -6.8578e-01, -1.9048e-01,\n",
       "          2.1629e-01,  7.1436e-01,  1.2732e+00,  6.8113e-01,  8.2347e-01,\n",
       "          1.9317e-01, -1.7887e+00,  8.2412e-01,  1.4823e-01, -9.5289e-01,\n",
       "         -4.4062e-01, -2.0423e+00,  3.7112e-01, -1.1648e+00,  5.3915e-01,\n",
       "         -4.0853e-01, -3.6751e-01, -1.9795e+00, -2.1988e+00, -5.8139e-01,\n",
       "          1.5124e+00, -6.3296e-01,  4.0252e-01, -6.2662e-01, -7.8445e-01,\n",
       "         -1.1986e+00, -1.4013e+00,  1.6040e+00,  7.9898e-01, -1.8496e+00,\n",
       "         -9.5292e-02, -5.4644e-01, -2.6673e-01, -8.3887e-03,  1.0275e-01,\n",
       "         -3.3965e-02,  1.3929e+00,  1.1860e+00,  1.3592e+00, -4.0571e-01,\n",
       "          2.8117e-01, -1.5254e+00, -1.2935e+00,  6.9387e-01, -7.0895e-01,\n",
       "          5.9410e-01,  2.0558e-01,  8.1241e-01, -1.2366e+00, -3.5584e-02,\n",
       "          1.5429e-01,  1.0291e+00,  5.6200e-01, -5.2386e-01,  1.2562e+00,\n",
       "         -6.4239e-01, -1.1461e+00, -1.0135e+00,  2.5589e-01, -2.3285e+00,\n",
       "         -4.6750e-01, -6.3005e-01, -1.2650e+00, -1.0708e+00, -7.4018e-01,\n",
       "         -1.4573e+00, -1.9181e+00, -4.2714e-01,  1.3922e+00, -1.3465e+00,\n",
       "          1.5137e+00, -5.2265e-01,  1.0011e-01, -7.1251e-01, -4.0522e-01,\n",
       "         -4.0887e-01, -8.8923e-01,  2.7927e-01,  6.6679e-01,  1.6785e+00,\n",
       "         -2.8674e-01, -1.0081e+00]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.w_embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google Colab Analog 2024 (PyTorch 2.5.1 + TensorFlow 2.18) [python-google_colab_gpu_2024]",
   "language": "python",
   "name": "conda-env-python-google_colab_gpu_2024-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
